import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import json
import os
import re
import time
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# --- Configuration ---
DATA_FILE = './psd_failures_cleaned_filtered.csv'
PARAMS_FILE = './component_regression_params.json'
INSIGHTS_FILE = './survival_insights_summary.csv'

# --- Column Names ---
COMPONENT_COL = 'Component'  # Korean component name
COMPONENT_EN_COL = 'Component_EN'  # English component name
LOCATION_COL = 'Location_Type_EN'  # Location type
STATION_COL = 'Station'  # Korean station name
STATION_EN_COL = 'Station_EN'  # English station name
STATION_RUNS_COL = 'Station_Daily_Runs'  # Continuous covariate

# --- Covariate Names ---
STATION_RUNS_STD_COEF = f"Q('{STATION_RUNS_COL}_std')"
LOCATION_UNDERGROUND_COEF = f"Q('{LOCATION_COL}_Underground')"
LOCATION_UNKNOWN_COEF = f"Q('{LOCATION_COL}_Unknown')"

# --- Time Horizons ---
TIME_HORIZONS_DAYS = [365, 365*2, 365*3, 365*5, 365*7, 365*10]
TIME_HORIZONS_LABELS = ["1 Year", "2 Years", "3 Years", "5 Years", "7 Years", "10 Years"]

# --- Translations ---
translations = {
    'en': {
        "page_title": "PSD Failure Analysis Dashboard",
        "dashboard_title": "ğŸšª PSD Failure Analysis Dashboard",
        "dashboard_subtitle": "Platform Screen Door Component Failure Probability Visualization",
        "loading_data": "Loading data...",
        "data_load_error": "Failed to load data. Please check the data files.",
        "sidebar_header": "Data Filters",
        "select_language": "Select Language:",
        "select_components": "Select Components:",
        "select_location_type": "Select Location Type:",
        "location_all": "All",
        "location_overall": "Overall",
        "location_above_ground": "Above Ground",
        "location_underground": "Underground",
        "tab_failure_curves": "Failure Curves",
        "tab_median_ttf": "Median TTF",
        "tab_custom_prediction": "Custom Prediction",
        "failure_curves_title": "Component Failure Probability Over Time",
        "failure_curves_desc": """**Methodology:** This chart visualizes how the probability of a component failing accumulates over time (in years). It uses a statistical technique called **Survival Analysis**, specifically the **Weibull Accelerated Failure Time (AFT) model**. This model is well-suited for understanding time-to-event data, like component failures.\n\n*   **Calculation:** The curves are generated from mathematical models fitted to historical failure data for each component type. Each model learns a baseline failure pattern (shape and scale parameters of the Weibull distribution) and how factors like **Location Type** and average **Station Daily Runs** influence the expected lifespan. The probability of failure by a certain time is calculated from these learned model parameters.\n*   **Interpretation:** A higher curve indicates a greater chance of failure occurring earlier. The steepness of the curve reflects how quickly the failure risk increases over time. Use the sidebar filters to compare specific components or focus on particular location types.""",
        "median_ttf_title": "Median Time to Failure Comparison",
        "median_ttf_desc": """**Methodology:** This chart compares the estimated **Median Time To Failure (Median TTF)** across different components and location types. Median TTF represents the estimated time by which 50% of components within a specific group are expected to have failed. It provides a typical lifespan estimate.\n\n*   **Calculation:** Median TTF is derived directly from the parameters of the same Weibull AFT models used for the failure curves. Specifically, it depends on the model's **shape parameter** (which describes the failure rate pattern) and its **scale parameter** (which represents the characteristic life). The scale parameter is adjusted based on the average characteristics (like daily runs) of the group being analyzed (e.g., 'Overall' represents the average across all locations for that component).\n*   **Interpretation:** Taller bars signify a longer typical operational lifespan before failure is expected. Comparing bars helps identify components or groups with significantly different expected longevities.""",
        "custom_prediction_title": "Custom Prediction Tool",
        "custom_prediction_desc": """**Methodology:** This tool allows you to generate a tailored failure prediction for a component under specific operating conditions that you define. It goes beyond the pre-calculated averages shown in the other tabs.\n\n*   **Calculation:** It starts with the base Weibull AFT model established for the selected component. Then, it **adjusts the model's parameters** (specifically, the scale or characteristic life) based on the **exact Location Type** and **Station Daily Runs** you input. This adjustment uses the relationships (coefficients) the model learned during its training on historical data, quantifying how much these specific factors accelerate or decelerate the time to failure compared to the baseline.\n*   **Usage:** Input the characteristics of a specific scenario (e.g., a particular high-traffic underground station). The tool then calculates and displays the resulting failure probability curve and Median TTF estimate for that precise case, providing a more granular risk assessment.\n\n**Important Limitation Note:** The current model shows that higher **Station Daily Runs** are associated with *longer* times to failure (higher Median TTF). This is counter-intuitive, as higher usage would typically be expected to lead to earlier failures. This likely occurs because the model does not account for **confounding factors**, such as **maintenance practices** (stations with higher usage might receive more frequent or better maintenance) or **station/component age** (newer stations might have both higher usage and more reliable components). Therefore, predictions heavily influenced by the 'Daily Runs' input should be interpreted with caution, as they may not fully reflect the real-world impact of usage without considering these other unmeasured factors.""",
        "find_station_title": "Find a Station",
        "search_station_label": "Search for a station (Korean or English name):",
        "matching_stations_title": "Matching Stations",
        "station_kr_name": "Korean Name",
        "station_en_name": "English Name",
        "station_daily_runs": "Daily Runs",
        "no_stations_found": "No stations found matching your search.",
        "configure_prediction_title": "Configure Prediction",
        "select_component_label": "Select Component:",
        "select_location_type_label": "Select Location Type:",
        "daily_runs_label": "Daily Runs:",
        "daily_runs_help": "Average number of train runs per day at the station",
        "generate_prediction_button": "Generate Prediction",
        "calculating_prediction": "Calculating prediction...",
        "failure_probabilities_title": "Failure Probabilities",
        "median_ttf_metric_label": "Estimated Median Time to Failure",
        "median_ttf_metric_unit_days": "days",
        "median_ttf_metric_unit_years": "years",
        "no_data_warning": "No data available for the selected filters.",
        "years_axis_label": "Years",
        "failure_prob_axis_label": "Failure Probability",
        "component_axis_label": "Component",
        "median_ttf_days_axis_label": "Median Time to Failure (Days)",
        "location_type_legend_label": "Location Type",
        "days_axis_label": "Days",
        "hover_failure_prob": "failure probability at",
        "hover_years": "years",
        "custom_pred_plot_title": "Custom Failure Prediction for",
        "no_model_warning": "No model parameters available for",
        # Time horizon labels
        "1_year": "1 Year", "2_years": "2 Years", "3_years": "3 Years", "5_years": "5 Years", "7_years": "7 Years", "10_years": "10 Years"
    },
    'ko': {
        "page_title": "PSD ê³ ì¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
        "dashboard_title": "ğŸšª PSD ê³ ì¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
        "dashboard_subtitle": "ìŠ¹ê°•ì¥ ìŠ¤í¬ë¦°ë„ì–´ êµ¬ì„±ìš”ì†Œ ê³ ì¥ í™•ë¥  ì‹œê°í™”",
        "loading_data": "ë°ì´í„° ë¡œë”© ì¤‘...",
        "data_load_error": "ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•˜ì‹­ì‹œì˜¤.",
        "sidebar_header": "ë°ì´í„° í•„í„°",
        "select_language": "ì–¸ì–´ ì„ íƒ:",
        "select_components": "êµ¬ì„±ìš”ì†Œ ì„ íƒ:",
        "select_location_type": "ìœ„ì¹˜ ìœ í˜• ì„ íƒ:",
        "location_all": "ì „ì²´",
        "location_overall": "ì „ì²´ í‰ê· ",
        "location_above_ground": "ì§€ìƒ",
        "location_underground": "ì§€í•˜",
        "tab_failure_curves": "ê³ ì¥ í™•ë¥  ê³¡ì„ ",
        "tab_median_ttf": "ê³ ì¥ê¹Œì§€ì˜ ì¤‘ìœ„ ì‹œê°„ (Median TTF)",
        "tab_custom_prediction": "ì‚¬ìš©ì ì •ì˜ ì˜ˆì¸¡",
        "failure_curves_title": "ì‹œê°„ì— ë”°ë¥¸ êµ¬ì„±ìš”ì†Œ ê³ ì¥ í™•ë¥ ",
        "failure_curves_desc": """**ë°©ë²•ë¡ :** ì´ ì°¨íŠ¸ëŠ” íŠ¹ì • ì‹œê°„(ë…„)ê¹Œì§€ êµ¬ì„±ìš”ì†Œê°€ ê³ ì¥ë‚  ëˆ„ì  í™•ë¥ ì„ ì‹œê°í™”í•©ë‹ˆë‹¤. **ìƒì¡´ ë¶„ì„**ì´ë¼ëŠ” í†µê³„ ê¸°ë²•, íŠ¹íˆ **Weibull ê°€ì† ìˆ˜ëª… ì‹œê°„(AFT) ëª¨ë¸**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ êµ¬ì„±ìš”ì†Œ ê³ ì¥ê³¼ ê°™ì€ ì‹œê°„-ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ ì´í•´í•˜ëŠ” ë° ì í•©í•©ë‹ˆë‹¤.\n\n*   **ê³„ì‚°:** ê³¡ì„ ì€ ê° êµ¬ì„±ìš”ì†Œ ìœ í˜•ì˜ ê³¼ê±° ê³ ì¥ ë°ì´í„°ì— ë§ì¶°ì§„ ìˆ˜í•™ì  ëª¨ë¸ì—ì„œ ìƒì„±ë©ë‹ˆë‹¤. ê° ëª¨ë¸ì€ ê¸°ì¤€ ê³ ì¥ íŒ¨í„´(Weibull ë¶„í¬ì˜ í˜•íƒœ ë° ì²™ë„ ëª¨ìˆ˜)ê³¼ **ìœ„ì¹˜ ìœ í˜•** ë° í‰ê·  **ì—­ë³„ ì¼ì¼ ìš´í–‰ íšŸìˆ˜**ì™€ ê°™ì€ ìš”ì¸ì´ ì˜ˆìƒ ìˆ˜ëª…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í•™ìŠµí•©ë‹ˆë‹¤. íŠ¹ì • ì‹œê°„ê¹Œì§€ì˜ ê³ ì¥ í™•ë¥ ì€ ì´ëŸ¬í•œ í•™ìŠµëœ ëª¨ë¸ ëª¨ìˆ˜ì—ì„œ ê³„ì‚°ë©ë‹ˆë‹¤.\n*   **í•´ì„:** ê³¡ì„ ì´ ë†’ì„ìˆ˜ë¡ ì¡°ê¸° ê³ ì¥ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê³¡ì„ ì˜ ê¸°ìš¸ê¸°ê°€ ê°€íŒŒë¥¼ìˆ˜ë¡ ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ê³ ì¥ ìœ„í—˜ ì¦ê°€ ì†ë„ê°€ ë¹ ë¥´ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì‚¬ì´ë“œë°” í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • êµ¬ì„±ìš”ì†Œë¥¼ ë¹„êµí•˜ê±°ë‚˜ íŠ¹ì • ìœ„ì¹˜ ìœ í˜•ì— ì´ˆì ì„ ë§ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.""",
        "median_ttf_title": "ê³ ì¥ê¹Œì§€ì˜ ì¤‘ìœ„ ì‹œê°„ ë¹„êµ",
        "median_ttf_desc": """**ë°©ë²•ë¡ :** ì´ ì°¨íŠ¸ëŠ” ë‹¤ì–‘í•œ êµ¬ì„±ìš”ì†Œ ë° ìœ„ì¹˜ ìœ í˜•ì— ê±¸ì³ ì˜ˆìƒë˜ëŠ” **ê³ ì¥ê¹Œì§€ì˜ ì¤‘ìœ„ ì‹œê°„(Median TTF)**ì„ ë¹„êµí•©ë‹ˆë‹¤. Median TTFëŠ” íŠ¹ì • ê·¸ë£¹ ë‚´ êµ¬ì„±ìš”ì†Œì˜ 50%ê°€ ê³ ì¥ë‚  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì‹œê°„ì„ ë‚˜íƒ€ë‚´ë©°, ì¼ë°˜ì ì¸ ìˆ˜ëª… ì¶”ì •ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n*   **ê³„ì‚°:** Median TTFëŠ” ê³ ì¥ ê³¡ì„ ì— ì‚¬ìš©ëœ ê²ƒê³¼ ë™ì¼í•œ Weibull AFT ëª¨ë¸ì˜ ëª¨ìˆ˜ì—ì„œ ì§ì ‘ íŒŒìƒë©ë‹ˆë‹¤. íŠ¹íˆ ëª¨ë¸ì˜ **í˜•íƒœ ëª¨ìˆ˜**(ê³ ì¥ë¥  íŒ¨í„´ ì„¤ëª…)ì™€ **ì²™ë„ ëª¨ìˆ˜**(íŠ¹ì„± ìˆ˜ëª… ë‚˜íƒ€ëƒ„)ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì²™ë„ ëª¨ìˆ˜ëŠ” ë¶„ì„ë˜ëŠ” ê·¸ë£¹(ì˜ˆ: 'ì „ì²´ í‰ê· 'ì€ í•´ë‹¹ êµ¬ì„±ìš”ì†Œì˜ ëª¨ë“  ìœ„ì¹˜ì— ëŒ€í•œ í‰ê· ì„ ë‚˜íƒ€ëƒ„)ì˜ í‰ê·  íŠ¹ì„±(ì˜ˆ: ì¼ì¼ ìš´í–‰ íšŸìˆ˜)ì„ ê¸°ë°˜ìœ¼ë¡œ ì¡°ì •ë©ë‹ˆë‹¤.\n*   **í•´ì„:** ë§‰ëŒ€ê°€ ë†’ì„ìˆ˜ë¡ ê³ ì¥ ì „ì— ì˜ˆìƒë˜ëŠ” ì¼ë°˜ì ì¸ ì‘ë™ ìˆ˜ëª…ì´ ê¸¸ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§‰ëŒ€ë¥¼ ë¹„êµí•˜ë©´ ì˜ˆìƒ ìˆ˜ëª…ì´ í¬ê²Œ ë‹¤ë¥¸ êµ¬ì„±ìš”ì†Œ ë˜ëŠ” ê·¸ë£¹ì„ ì‹ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.""",
        "custom_prediction_title": "ì‚¬ìš©ì ì •ì˜ ì˜ˆì¸¡ ë„êµ¬",
        "custom_prediction_desc": """**ë°©ë²•ë¡ :** ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì •ì˜í•œ íŠ¹ì • ìš´ì˜ ì¡°ê±´ì—ì„œ êµ¬ì„±ìš”ì†Œì— ëŒ€í•œ ë§ì¶¤í˜• ê³ ì¥ ì˜ˆì¸¡ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íƒ­ì— í‘œì‹œëœ ë¯¸ë¦¬ ê³„ì‚°ëœ í‰ê· ê°’ì„ ë„˜ì–´ì„­ë‹ˆë‹¤.\n\n*   **ê³„ì‚°:** ì„ íƒí•œ êµ¬ì„±ìš”ì†Œì— ëŒ€í•´ ì„¤ì •ëœ ê¸°ë³¸ Weibull AFT ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì…ë ¥í•œ **ì •í™•í•œ ìœ„ì¹˜ ìœ í˜•** ë° **ì—­ë³„ ì¼ì¼ ìš´í–‰ íšŸìˆ˜**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì˜ ëª¨ìˆ˜(íŠ¹íˆ ì²™ë„ ë˜ëŠ” íŠ¹ì„± ìˆ˜ëª…)ë¥¼ **ì¡°ì •**í•©ë‹ˆë‹¤. ì´ ì¡°ì •ì€ ëª¨ë¸ì´ ê³¼ê±° ë°ì´í„° í•™ìŠµ ì¤‘ì— í•™ìŠµí•œ ê´€ê³„(ê³„ìˆ˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ íŠ¹ì • ìš”ì¸ì´ ê¸°ì¤€ì„ ê³¼ ë¹„êµí•˜ì—¬ ê³ ì¥ê¹Œì§€ì˜ ì‹œê°„ì„ ì–¼ë§ˆë‚˜ ê°€ì† ë˜ëŠ” ê°ì†ì‹œí‚¤ëŠ”ì§€ë¥¼ ì •ëŸ‰í™”í•©ë‹ˆë‹¤.\n*   **ì‚¬ìš©ë²•:** íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤(ì˜ˆ: íŠ¹ì • êµí†µëŸ‰ì´ ë§ì€ ì§€í•˜ì—­)ì˜ íŠ¹ì„±ì„ ì…ë ¥í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë„êµ¬ëŠ” í•´ë‹¹ íŠ¹ì • ì‚¬ë¡€ì— ëŒ€í•œ ê²°ê³¼ì ì¸ ê³ ì¥ í™•ë¥  ê³¡ì„  ë° Median TTF ì¶”ì •ì¹˜ë¥¼ ê³„ì‚°í•˜ê³  í‘œì‹œí•˜ì—¬ ë³´ë‹¤ ì„¸ë¶„í™”ëœ ìœ„í—˜ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n**ì¤‘ìš” ì œí•œì‚¬í•­ ì°¸ê³ :** í˜„ì¬ ëª¨ë¸ì€ **ì—­ë³„ ì¼ì¼ ìš´í–‰ íšŸìˆ˜**ê°€ ë†’ì„ìˆ˜ë¡ ê³ ì¥ê¹Œì§€ì˜ ì‹œê°„(ë” ë†’ì€ Median TTF)ì´ *ê¸¸ì–´ì§€ëŠ”* ì—°ê´€ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ëŸ‰ì´ ë§ì„ìˆ˜ë¡ ì¡°ê¸° ê³ ì¥ìœ¼ë¡œ ì´ì–´ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ê¸° ë•Œë¬¸ì— ì§ê´€ì— ë°˜í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ **êµë€ ë³€ìˆ˜**ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, **ìœ ì§€ë³´ìˆ˜ ê´€í–‰**(ì‚¬ìš©ëŸ‰ì´ ë§ì€ ì—­ì´ ë” ë¹ˆë²ˆí•˜ê±°ë‚˜ ë” ë‚˜ì€ ìœ ì§€ë³´ìˆ˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ) ë˜ëŠ” **ì—­/êµ¬ì„±ìš”ì†Œ ë…¸í›„ë„**(ì‹ ê·œ ì—­ì€ ì‚¬ìš©ëŸ‰ì´ ë§ê³  ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” êµ¬ì„±ìš”ì†Œë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ) ë“±ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ 'ì¼ì¼ ìš´í–‰ íšŸìˆ˜' ì…ë ¥ì— í¬ê²Œ ì˜í–¥ì„ ë°›ëŠ” ì˜ˆì¸¡ì€ ì´ëŸ¬í•œ ì¸¡ì •ë˜ì§€ ì•Šì€ ë‹¤ë¥¸ ìš”ì¸ì„ ê³ ë ¤í•˜ì§€ ì•Šê³  ì‚¬ìš©ëŸ‰ì˜ ì‹¤ì œ ì˜í–¥ì„ ì™„ì „íˆ ë°˜ì˜í•˜ì§€ ëª»í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•´ì„œ í•´ì„í•´ì•¼ í•©ë‹ˆë‹¤.""",
        "find_station_title": "ì—­ ì°¾ê¸°",
        "search_station_label": "ì—­ ê²€ìƒ‰ (í•œê¸€ ë˜ëŠ” ì˜ë¬¸ëª…):",
        "matching_stations_title": "ì¼ì¹˜í•˜ëŠ” ì—­",
        "station_kr_name": "í•œê¸€ ì—­ëª…",
        "station_en_name": "ì˜ë¬¸ ì—­ëª…",
        "station_daily_runs": "ì¼ì¼ ìš´í–‰ íšŸìˆ˜",
        "no_stations_found": "ê²€ìƒ‰ê³¼ ì¼ì¹˜í•˜ëŠ” ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "configure_prediction_title": "ì˜ˆì¸¡ êµ¬ì„±",
        "select_component_label": "êµ¬ì„±ìš”ì†Œ ì„ íƒ:",
        "select_location_type_label": "ìœ„ì¹˜ ìœ í˜• ì„ íƒ:",
        "daily_runs_label": "ì¼ì¼ ìš´í–‰ íšŸìˆ˜:",
        "daily_runs_help": "ì—­ì˜ ì¼ì¼ í‰ê·  ì—´ì°¨ ìš´í–‰ íšŸìˆ˜",
        "generate_prediction_button": "ì˜ˆì¸¡ ìƒì„±",
        "calculating_prediction": "ì˜ˆì¸¡ ê³„ì‚° ì¤‘...",
        "failure_probabilities_title": "ê³ ì¥ í™•ë¥ ",
        "median_ttf_metric_label": "ì˜ˆìƒ ê³ ì¥ê¹Œì§€ì˜ ì¤‘ìœ„ ì‹œê°„",
        "median_ttf_metric_unit_days": "ì¼",
        "median_ttf_metric_unit_years": "ë…„",
        "no_data_warning": "ì„ íƒí•œ í•„í„°ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
        "years_axis_label": "ë…„",
        "failure_prob_axis_label": "ê³ ì¥ í™•ë¥ ",
        "component_axis_label": "êµ¬ì„±ìš”ì†Œ",
        "median_ttf_days_axis_label": "ê³ ì¥ê¹Œì§€ì˜ ì¤‘ìœ„ ì‹œê°„ (ì¼)",
        "location_type_legend_label": "ìœ„ì¹˜ ìœ í˜•",
        "days_axis_label": "ì¼",
        "hover_failure_prob": "ê³ ì¥ í™•ë¥ ",
        "hover_years": "ë…„",
        "custom_pred_plot_title": "ì‚¬ìš©ì ì •ì˜ ê³ ì¥ ì˜ˆì¸¡:",
        "no_model_warning": "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ì—†ìŒ:",
        # Time horizon labels (Korean)
        "1_year": "1ë…„", "2_years": "2ë…„", "3_years": "3ë…„", "5_years": "5ë…„", "7_years": "7ë…„", "10_years": "10ë…„"
    }
}

# --- Helper Functions ---

@st.cache_data
def load_data():
    """Load and prepare all necessary data files."""
    try:
        # Load main data file
        df = pd.read_csv(DATA_FILE, low_memory=False)
        
        # Load insights summary
        insights_df = pd.read_csv(INSIGHTS_FILE)
        
        # Load model parameters
        with open(PARAMS_FILE, 'r') as f:
            params_data = json.load(f)
        
        return df, insights_df, params_data
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None, None, None

def calculate_survival_prob(shape, scale, horizon_days):
    """Calculates survival probability using Weibull CDF."""
    if shape <= 0 or scale <= 0 or np.isnan(shape) or np.isnan(scale):
        return np.nan
    try:
        fail_prob = stats.weibull_min.cdf(float(horizon_days), c=shape, scale=scale)
        return 1.0 - fail_prob
    except Exception as e:
        return np.nan

def calculate_median_ttf(shape, scale):
    """Calculates median Time To Failure for Weibull."""
    if shape <= 0 or scale <= 0 or np.isnan(shape) or np.isnan(scale):
        return np.nan
    try:
        # Median = scale * (ln(2))^(1/shape)
        return scale * (np.log(2)**(1/shape))
    except Exception as e:
        return np.nan

def adjust_scale_for_covariates(base_log_lambda, coefficients, scenario, std_stats):
    """
    Adjusts the Weibull scale parameter based on scenario covariates.
    """
    log_lambda = base_log_lambda

    # 1. Handle Station_Daily_Runs (Standardized Continuous)
    if STATION_RUNS_STD_COEF in coefficients and STATION_RUNS_COL in std_stats:
        mean = std_stats[STATION_RUNS_COL].get('mean', 0)
        std = std_stats[STATION_RUNS_COL].get('std', 1)
        if std > 0:
            raw_value = scenario.get(STATION_RUNS_COL, mean)
            standardized_value = (raw_value - mean) / std
            applied_coef = coefficients[STATION_RUNS_STD_COEF]
            lambda_change = applied_coef * standardized_value
            log_lambda += lambda_change

    # 2. Handle Location_Type_EN (Categorical Dummy)
    current_location = scenario.get(LOCATION_COL)
    
    # Apply Underground coefficient if applicable
    if current_location == 'Underground' and LOCATION_UNDERGROUND_COEF in coefficients:
        log_lambda += coefficients[LOCATION_UNDERGROUND_COEF] * 1
        
    # Apply Unknown coefficient if applicable
    elif current_location == 'Unknown' and LOCATION_UNKNOWN_COEF in coefficients:
        log_lambda += coefficients[LOCATION_UNKNOWN_COEF] * 1

    return np.exp(log_lambda)

def calculate_custom_survival_probabilities(component_name, station_runs, location_type, params_data):
    """
    Calculate custom survival probabilities for a component based on station runs and location.
    """
    # Get component parameters
    component_params = params_data['component_models'].get(component_name, None)
    std_stats = params_data['standardization_stats']
    
    if component_params is None:
        return None
    
    # Get model parameters
    log_shape = component_params.get('log_rho')
    base_log_lambda = component_params.get('log_lambda')
    coefficients = component_params.get('coef', {})
    
    if log_shape is None or base_log_lambda is None:
        return None
    
    # Calculate actual Weibull shape parameter
    actual_shape = np.exp(log_shape)
    
    # Create scenario
    scenario = {
        STATION_RUNS_COL: station_runs,
        LOCATION_COL: location_type
    }
    
    # Adjust scale parameter for covariates
    adjusted_scale = adjust_scale_for_covariates(base_log_lambda, coefficients, scenario, std_stats)
    
    # Calculate median time to failure
    median_ttf = calculate_median_ttf(actual_shape, adjusted_scale)
    
    # Calculate survival probabilities for each time horizon
    results = {'Median_TTF_Days': median_ttf}
    for horizon in TIME_HORIZONS_DAYS:
        surv_prob = calculate_survival_prob(actual_shape, adjusted_scale, horizon)
        results[f'Survival_Prob_{horizon}d'] = surv_prob
    
    return results

def plot_failure_curves(filtered_insights_df, lang, components=None, location_type=None):
    """
    Plot failure probability curves for selected components and location type.
    Pass selected language `lang`.
    """
    # Determine component column based on language
    lang_component_col = COMPONENT_EN_COL if lang == 'en' else COMPONENT_COL
    display_df = filtered_insights_df.copy()
    # Add the language-specific component name for display if not present
    if lang_component_col not in display_df.columns and (COMPONENT_COL in display_df.columns and COMPONENT_EN_COL in display_df.columns):
         # Simple mapping assuming EN_COL is the key
         comp_map = display_df.set_index(COMPONENT_EN_COL)[COMPONENT_COL].to_dict()
         display_df[lang_component_col] = display_df[COMPONENT_EN_COL].map(comp_map)
         
    # Translate Location Type for display
    loc_map = {
        'Overall': translations[lang]['location_overall'],
        'Above Ground': translations[lang]['location_above_ground'],
        'Underground': translations[lang]['location_underground']
    }
    display_df['Location_Display'] = display_df[LOCATION_COL].map(loc_map).fillna(display_df[LOCATION_COL]) # Keep original if no map
    
    if components:
        # Filter based on internal English name
        df = display_df[display_df[COMPONENT_EN_COL].isin(components)]
    else:
        df = display_df

    # CORRECTED: Check against internal key "All" for filtering logic
    if location_type and location_type != "All": 
         # Filter based on English key for location type
         # We receive the internal key ('All', 'Overall', 'Above Ground', 'Underground') from main()
         # No need to map back from display name here. Direct comparison is correct.
         df = df[df[LOCATION_COL] == location_type]
         # If location_type was 'All', this block is skipped.

    if df.empty:
        st.warning(translations[lang]['no_data_warning'])
        return

    fig = make_subplots(rows=1, cols=1)
    time_horizons_years = [d/365 for d in TIME_HORIZONS_DAYS]

    for component_en in df[COMPONENT_EN_COL].unique(): # Iterate using English key
        component_df = df[df[COMPONENT_EN_COL] == component_en]
        # Get the display name (English or Korean)
        component_display_name = component_df[lang_component_col].iloc[0] 

        for _, row in component_df.iterrows():
            location_display = row['Location_Display']
            failure_probs = [1 - row[f'Survival_Prob_{horizon}d'] for horizon in TIME_HORIZONS_DAYS]
            line_name = f"{component_display_name} - {location_display}"

            fig.add_trace(
                go.Scatter(
                    x=time_horizons_years,
                    y=failure_probs,
                    mode='lines+markers',
                    name=line_name,
                    hovertemplate=f"<b>%{{y:.2%}}</b> {translations[lang]['hover_failure_prob']} %{{x:.1f}} {translations[lang]['hover_years']}<extra></extra>"
                )
            )

    fig.update_layout(
        title=translations[lang]['failure_curves_title'],
        xaxis_title=translations[lang]['years_axis_label'],
        yaxis_title=translations[lang]['failure_prob_axis_label'],
        yaxis=dict(tickformat=".0%", range=[0, 1]),
        legend=dict(orientation="h", yanchor="bottom", y=-0.3, xanchor="center", x=0.5, title=translations[lang]['location_type_legend_label']),
        height=600,
        hovermode="closest"
    )
    return fig

def plot_ttf_comparison(filtered_insights_df, lang, components=None, location_type=None):
    """
    Create a bar chart comparing median time to failure.
    Pass selected language `lang`.
    """
    # Determine component column based on language
    lang_component_col = COMPONENT_EN_COL if lang == 'en' else COMPONENT_COL
    display_df = filtered_insights_df.copy()
    # Add the language-specific component name for display if not present
    if lang_component_col not in display_df.columns and (COMPONENT_COL in display_df.columns and COMPONENT_EN_COL in display_df.columns):
         comp_map = display_df.set_index(COMPONENT_EN_COL)[COMPONENT_COL].to_dict()
         display_df[lang_component_col] = display_df[COMPONENT_EN_COL].map(comp_map)
         
    # Translate Location Type for display
    loc_map = {
        'Overall': translations[lang]['location_overall'],
        'Above Ground': translations[lang]['location_above_ground'],
        'Underground': translations[lang]['location_underground']
    }
    display_df['Location_Display'] = display_df[LOCATION_COL].map(loc_map).fillna(display_df[LOCATION_COL])

    if components:
        # Filter based on internal English name
        df = display_df[display_df[COMPONENT_EN_COL].isin(components)]
    else:
        df = display_df

    # CORRECTED: Check against internal key "All" for filtering logic
    if location_type and location_type != "All": 
         # Filter based on English key for location type
         # We receive the internal key ('All', 'Overall', 'Above Ground', 'Underground') from main()
         # No need to map back from display name here. Direct comparison is correct.
         df = df[df[LOCATION_COL] == location_type]
         # If location_type was 'All', this block is skipped.

    if df.empty:
        st.warning(translations[lang]['no_data_warning'])
        return

    fig = px.bar(
        df,
        x=lang_component_col, # Use language-specific component name for x-axis
        y='Median_TTF_Days',
        color='Location_Display', # Use translated location for color legend
        barmode='group',
        labels={
            lang_component_col: translations[lang]['component_axis_label'],
            'Median_TTF_Days': translations[lang]['median_ttf_days_axis_label'],
            'Location_Display': translations[lang]['location_type_legend_label']
        },
        title=translations[lang]['median_ttf_title'],
        height=500
    )

    fig.update_layout(
        yaxis=dict(title=translations[lang]['days_axis_label']),
        yaxis2=dict(
            title=translations[lang]['years_axis_label'],
            overlaying="y", side="right", showgrid=False,
            tickvals=[365, 730, 1095, 1825, 2555, 3650],
            ticktext=["1", "2", "3", "5", "7", "10"]
        ),
        legend_title_text=translations[lang]['location_type_legend_label']
    )
    return fig

def plot_custom_prediction(component_name, station_runs, location_type_key, params_data, lang):
    """
    Plot custom prediction for a specific component based on station runs and location.
    Pass selected language `lang` and the English `location_type_key`.
    """
    results = calculate_custom_survival_probabilities(component_name, station_runs, location_type_key, params_data)

    if results is None:
        st.warning(f"{translations[lang]['no_model_warning']} {component_name}")
        return None

    time_horizons_years = [d/365 for d in TIME_HORIZONS_DAYS]
    failure_probs = [1 - results[f'Survival_Prob_{horizon}d'] for horizon in TIME_HORIZONS_DAYS]

    # Translate component and location for title/legend
    # Assuming component_name is passed as English key
    comp_display_name = component_name # Keep EN for now, need main df access or param file change for KO name
    loc_display_name = translations[lang].get(f'location_{location_type_key.lower().replace(" ", "_")}', location_type_key)

    fig = go.Figure()
    fig.add_trace(
        go.Scatter(
            x=time_horizons_years,
            y=failure_probs,
            mode='lines+markers',
            name=f"{comp_display_name} - {loc_display_name}",
            hovertemplate=f"<b>%{{y:.2%}}</b> {translations[lang]['hover_failure_prob']} %{{x:.1f}} {translations[lang]['hover_years']}<extra></extra>"
        )
    )

    plot_title = f"{translations[lang]['custom_pred_plot_title']} {comp_display_name} ({loc_display_name}, {station_runs} {translations[lang]['daily_runs_label']})"
    fig.update_layout(
        title=plot_title,
        xaxis_title=translations[lang]['years_axis_label'],
        yaxis_title=translations[lang]['failure_prob_axis_label'],
        yaxis=dict(tickformat=".0%", range=[0, 1]),
        height=500,
        hovermode="closest"
    )

    for horizon, prob in zip(TIME_HORIZONS_DAYS, failure_probs):
        results[f'Failure_Prob_{horizon}d'] = prob

    return fig, results

# --- Main Dashboard ---
def main():
    # Initialize session state for language if it doesn't exist
    if 'language' not in st.session_state:
        st.session_state.language = 'en' # Default to English
        
    # Get current language
    lang = st.session_state.language
    
    # Set page config - must be the first Streamlit command
    st.set_page_config(
        page_title=translations[lang]["page_title"],
        page_icon="ğŸšª",
        layout="wide"
    )
    
    # --- Language Selector --- 
    # Place it early, maybe at the top of the sidebar
    st.sidebar.radio(
        label=translations[lang]["select_language"],
        options=['en', 'ko'],
        format_func=lambda x: "English" if x == 'en' else "í•œêµ­ì–´",
        key='language', # Link to session state key
        horizontal=True,
    )
    # Update lang variable after potential change from radio button
    lang = st.session_state.language 
    
    st.title(translations[lang]["dashboard_title"])
    st.markdown(f"### {translations[lang]['dashboard_subtitle']}")
    
    # Load all data
    with st.spinner(translations[lang]["loading_data"]):
        df, insights_df, params_data = load_data()
    
    if df is None or insights_df is None or params_data is None:
        st.error(translations[lang]["data_load_error"])
        return
    
    # Filter out 'Unknown' location type from insights for display
    display_insights_df = insights_df[insights_df[LOCATION_COL] != 'Unknown'].copy()

    # Add Korean component name if missing (needed for display options)
    if COMPONENT_COL not in display_insights_df.columns and COMPONENT_EN_COL in display_insights_df.columns and COMPONENT_COL in df.columns:
         comp_map = df[[COMPONENT_EN_COL, COMPONENT_COL]].drop_duplicates().set_index(COMPONENT_EN_COL)[COMPONENT_COL].to_dict()
         display_insights_df[COMPONENT_COL] = display_insights_df[COMPONENT_EN_COL].map(comp_map)

    # Sidebar filters
    st.sidebar.header(translations[lang]["sidebar_header"])
    
    # --- Component Selection Logic --- 
    # 1. Get unique English component names from the insights data
    available_components_en = sorted(display_insights_df[COMPONENT_EN_COL].unique())
    
    # 2. Create the mapping from EN to KR using the main dataframe `df`
    component_en_to_kr_map = {}
    if COMPONENT_COL in df.columns and COMPONENT_EN_COL in df.columns:
        component_en_to_kr_map = df[[COMPONENT_EN_COL, COMPONENT_COL]].drop_duplicates().set_index(COMPONENT_EN_COL)[COMPONENT_COL].to_dict()
        # Ensure all available EN components have a mapping, use EN name as fallback
        for en_name in available_components_en:
            if en_name not in component_en_to_kr_map:
                 component_en_to_kr_map[en_name] = en_name # Fallback if KR name missing in main df
    else: # Fallback if KR column doesn't exist in main df
        component_en_to_kr_map = {en_name: en_name for en_name in available_components_en}

    # 3. Determine the display options and create mapping back to EN key
    if lang == 'ko':
        # Create KR options, ensure order matches available_components_en
        available_components_display = [component_en_to_kr_map.get(en_name, en_name) for en_name in available_components_en]
        # Map displayed KR name back to EN key
        component_display_to_en_map = {kr_name: en_name for en_name, kr_name in component_en_to_kr_map.items() if en_name in available_components_en}
    else: # lang == 'en'
        available_components_display = available_components_en
        # Map displayed EN name back to EN key (identity map)
        component_display_to_en_map = {en_name: en_name for en_name in available_components_en}

    # 4. Populate the multiselect widget
    selected_components_display = st.sidebar.multiselect(
        translations[lang]["select_components"],
        options=sorted(available_components_display), # Sort display names for the user
        default=sorted(available_components_display)[:3] 
    )
    
    # 5. Convert selected display names back to internal English keys for filtering
    selected_components = [component_display_to_en_map[disp_name] for disp_name in selected_components_display if disp_name in component_display_to_en_map]
    # --- End Component Selection Logic ---

    # Location type selection - Generate options from filtered data
    loc_display_map = {
        'All': translations[lang]['location_all'],
        'Overall': translations[lang]['location_overall'],
        'Above Ground': translations[lang]['location_above_ground'],
        'Underground': translations[lang]['location_underground']
    }
    available_location_keys = sorted(display_insights_df[display_insights_df[LOCATION_COL] != 'Overall'][LOCATION_COL].unique().tolist())
    location_display_options = [loc_display_map['All'], loc_display_map['Overall']] + [loc_display_map[key] for key in available_location_keys if key in loc_display_map]
    
    selected_location_display = st.sidebar.selectbox(
        translations[lang]["select_location_type"],
        options=location_display_options,
        index=0 
    )
    # Convert selected display location back to internal key or display name itself if 'All'
    selected_location = next((key for key, value in loc_display_map.items() if value == selected_location_display), selected_location_display)

    # Filter insights data based on selections
    filtered_display_data = display_insights_df.copy()
    if selected_components: # Use internal English keys for filtering
        filtered_display_data = filtered_display_data[filtered_display_data[COMPONENT_EN_COL].isin(selected_components)]
    if selected_location != "All": # Use internal English keys for filtering
        filtered_display_data = filtered_display_data[filtered_display_data[LOCATION_COL] == selected_location]
    
    # Map time horizon labels
    time_horizon_labels_map = {
        "1 Year": translations[lang]["1_year"], "2 Years": translations[lang]["2_years"], "3 Years": translations[lang]["3_years"],
        "5 Years": translations[lang]["5_years"], "7 Years": translations[lang]["7_years"], "10 Years": translations[lang]["10_years"]
    }
    time_horizon_labels_display = [time_horizon_labels_map.get(lbl, lbl) for lbl in TIME_HORIZONS_LABELS]

    # Tabs for different visualizations
    tab_labels = [translations[lang]["tab_failure_curves"], translations[lang]["tab_median_ttf"], translations[lang]["tab_custom_prediction"]]
    tab1, tab2, tab3 = st.tabs(tab_labels)

    # Tab 1: Failure curves over time
    with tab1:
        st.markdown(f"### {translations[lang]['failure_curves_title']}")
        st.markdown(translations[lang]['failure_curves_desc'])

        if filtered_display_data.empty:
            st.warning(translations[lang]['no_data_warning'])
        else:
            fig = plot_failure_curves(filtered_display_data, lang, selected_components, selected_location)
            if fig:
                st.plotly_chart(fig, use_container_width=True)

    # Tab 2: Median Time to Failure comparison
    with tab2:
        st.markdown(f"### {translations[lang]['median_ttf_title']}")
        st.markdown(translations[lang]['median_ttf_desc'])

        if filtered_display_data.empty:
            st.warning(translations[lang]['no_data_warning'])
        else:
            fig = plot_ttf_comparison(filtered_display_data, lang, selected_components, selected_location)
            if fig:
                st.plotly_chart(fig, use_container_width=True)

    # Tab 3: Custom prediction based on station runs
    with tab3:
        st.markdown(f"### {translations[lang]['custom_prediction_title']}")
        st.markdown(translations[lang]['custom_prediction_desc'])

        # Station search - with Korean and English names
        st.markdown(f"#### {translations[lang]['find_station_title']}")
        search_query = st.text_input(translations[lang]["search_station_label"], "")

        stations_info = df[[STATION_COL, STATION_EN_COL, STATION_RUNS_COL]].drop_duplicates()

        if search_query:
            filtered_stations = stations_info[
                stations_info[STATION_COL].str.contains(search_query, case=False, na=False) | 
                stations_info[STATION_EN_COL].str.contains(search_query, case=False, na=False)
            ]

            if not filtered_stations.empty:
                st.markdown(f"#### {translations[lang]['matching_stations_title']}")
                # Choose display columns based on language
                station_display_cols = {
                     STATION_COL: translations[lang]["station_kr_name"],
                     STATION_EN_COL: translations[lang]["station_en_name"],
                     STATION_RUNS_COL: translations[lang]["station_daily_runs"]
                 }
                st.dataframe(
                    filtered_stations.rename(columns=station_display_cols),
                    hide_index=True
                )
            else:
                st.warning(translations[lang]["no_stations_found"])

        st.markdown(f"#### {translations[lang]['configure_prediction_title']}")
        col1, col2, col3 = st.columns(3)

        with col1:
            # Use display names for options, map back to EN key
            custom_component_display = st.selectbox(
                translations[lang]["select_component_label"],
                options=available_components_display
            )
            custom_component = component_display_to_en_map.get(custom_component_display, None)

        with col2:
             # Use translated location options, map back to EN key
            custom_location_options_map = {
                 translations[lang]['location_above_ground']: 'Above Ground',
                 translations[lang]['location_underground']: 'Underground'
             }
            custom_location_display = st.selectbox(
                translations[lang]["select_location_type_label"],
                options=list(custom_location_options_map.keys()),
                index=0
            )
            custom_location_key = custom_location_options_map.get(custom_location_display)

        with col3:
            mean_runs = params_data['standardization_stats'][STATION_RUNS_COL]['mean']
            custom_station_runs = st.number_input(
                translations[lang]["daily_runs_label"],
                min_value=0,
                max_value=1000, # Increased max value slightly
                value=int(mean_runs),
                help=translations[lang]["daily_runs_help"]
            )

        if st.button(translations[lang]["generate_prediction_button"]):
             if custom_component and custom_location_key:
                 with st.spinner(translations[lang]["calculating_prediction"]):
                     fig, results = plot_custom_prediction(
                         custom_component, 
                         custom_station_runs, 
                         custom_location_key,
                         params_data,
                         lang
                     )
                     
                     if fig and results:
                         st.plotly_chart(fig, use_container_width=True)
                         st.markdown(f"#### {translations[lang]['failure_probabilities_title']}")
                         metrics_col1, metrics_col2 = st.columns(2)
                         with metrics_col1:
                             st.metric(
                                 translations[lang]["median_ttf_metric_label"], 
                                 f"{results['Median_TTF_Days']:.1f} {translations[lang]['median_ttf_metric_unit_days']}",
                                 f"{results['Median_TTF_Days']/365:.1f} {translations[lang]['median_ttf_metric_unit_years']}"
                             )
                         prob_cols = st.columns(len(TIME_HORIZONS_DAYS))
                         for i, (horizon, label_key) in enumerate(zip(TIME_HORIZONS_DAYS, ["1_year", "2_years", "3_years", "5_years", "7_years", "10_years"])):
                             with prob_cols[i]:
                                 prob_value = results[f'Failure_Prob_{horizon}d'] 
                                 st.metric(
                                     translations[lang][label_key], 
                                     f"{prob_value:.1%}"
                                 )

if __name__ == "__main__":
    main() 